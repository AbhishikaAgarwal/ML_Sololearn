** Percentile **

50 Percentile is similar to median..
75 Percentile means only 25% of the data is greater than that..
And so on..



** Variance and standard deviation **

To calculate variance, we first calculate the mean of the data and then for all the values, we see the deviation from the mean.
Then we square all the values obtained by calculating deviation and then add them all, finally divide it by the number of terms.
This gives us the variance and its square root gives us the standard deviation.

Let's say the data is : 
15, 16, 18, 19, 22, 24, 29, 30, 34

Mean of the given set of values is 23

Let's calculate how far each value is from the mean. 15 is 8 away from the mean (since 23-15=8).

Now we get the values :  8, 7, 5, 4, 1, 1, 6, 7, 11

Then, 8^2 + 7^2 + 5^2 +..... + 11^2 = 362

Variance = 362/9 = 40.22

Standard deviation = sqrt(Variance) = 6.34




** Numpy **

import numpy as np

data = [15, 16, 18, 19, 22, 24, 29, 30, 34]

print("mean:", np.mean(data))
print("median:", np.median(data))
print("50th percentile (median):", np.percentile(data, 50))
print("25th percentile:", np.percentile(data, 25))
print("75th percentile:", np.percentile(data, 75))
print("standard deviation:", np.std(data))
print("variance:", np.var(data))



** Pandas

In pandas, we store data in csv file
Let's say our data is stored in 'titanic.csv' file and the data is :

Survived, Pclass, Sex, Age, Siblings/Spouses, Parents/Children, Fare
0, 3, male, 22.0, 1, 0, 7.25
1, 1, female, 38.0, 1, 0, 71.2833
1, 3, female, 26.0, 0, 0, 7.925
1, 1, female, 35.0, 1, 0, 53.1

The first line is the header and then each subsequent lines contains value for each person.


import pandas as pd
df = pd.read_csv('titanic.csv')		// df :- data frame
print(df.head)			// prints the header or headings
print(df.describe)			//It returns a table of statistics about the columns like count, mean,std,variance,percentiles(50,25,75) and so on

col = df['Fare']			// Here, we're selecting just the column with the passenger fares.
print(col)				// It is known as pandas series

// We can make a small dataframe from the original dataframe as follows :-

small_df = df[['Age', 'Sex', 'Survived']]	// Here we make a small data frame by selecting only some selected columns(Age,Sex,Survived)
print(small_df.head()) 

// We can also create a column according to the boolean value of some other column as follows :-

df['male'] = df['Sex'] == 'male'		// This creates a new column named male and stores "True" if Sex is male otherwise "False"

// We can convert any column(Series) into the array as well using the following

print(df['Fare'].values)	// Series into numpy array(1D)

print(df[['Pclass', 'Fare', 'Age']].values)	//Small data frame into numpy array(3D)

To get the number of rows and columns, we use shape

arr = df[['Pclass', 'Fare', 'Age']].values
print(arr.shape)	// It return the number of rows and colums of the small df or the 3D numpy array


arr = df[['Pclass', 'Fare', 'Age']].values
print(arr[0, 1])	// It the value at 1st row and 2nd column i.e. the fare of 1st passenger(Column 1 is of Fare and 0 means its first value)
print(arr[0])	// It return the complete first row ( Values of pclass, fare and age of 1st passenger)
print(arr[:,2])	// It returns the complete column2( Age column)


arr = df[['Pclass', 'Fare', 'Age']].values[:10]		// This returns the first 10 values of each of the 3 column for simplicity

mask = arr[:, 2] < 18			// This creates a mask array containing 1 where age<18 and 0 elsewhere
print(arr[mask])			// Print the array here
print(arr[arr[:, 2] < 18])		// Combination of above 2 lines

print((arr[:, 2] < 18).sum())		// If we sum up the values of the mask , we get the count of the people of age<18





** Matplotlib **

import matplotlib.pyplot as plt

plt.scatter(df['Age'], df['Fare'])		// First argument in the plot is for x-axis and second argument for y-axis

// Following 2 lines labels the x and y axis
plt.xlabel('Age')
plt.ylabel('Fare')

plt.scatter(df['Age'], df['Fare'], c=df['Pclass'])	// To give each of the 3 class different color, we assign it to a panda series

plt.plot([0, 80], [85, 5])		// We can plot a line (0,85) and (80,5) 





** Classification **

Machine Learning on a high level is made up of supervised and unsupervised learning.

Supervised Learning means that we will have labeled historical data that we will use to inform our model. 
We call the label or thing we’re trying to predict, the target. 
So in supervised learning, there is a known target for the historical data, and for unsupervised learning there is no known target.

Within supervised learning, there is Classification and Regression. 
Classification problems are where the target is a categorical value (often True or False, but can be multiple categories). 
Regression problems are where the target is a numerical value.

For example, predicting housing prices is a regression problem. 
It’s supervised, since we have historical data of the sales of houses in the past. 
It’s regression, because the housing price is a numerical value. 
Predicting if someone will default on their loan is a classification problem. 
Again, it’s supervised, since we have the historical data of whether past loanees defaulted, 
and it’s a classification problem because we are trying to predict if the loan is in one of two categories (default or not).

*Logistic Regression, while it has regression in its name is an algorithm for solving classification problems, not regression problems.



**Logistic Regression Model **

Sigmoid is the function for calculation of probability :- 1/(1+e^-(ax+by+c))
	

import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('titanic.csv')
X = df[['Fare', 'Age']].values
y = df['Survived'].values

model = LogisticRegression()
model.fit(X, y)

print(model.coef_, model.intercept_)
# [[ 0.01615949 -0.01549065]] [-0.51037152]

These values mean that the equation is as follows:


import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

model = LogisticRegression()
model.fit(X, y)

print(model.predict([[3, True, 22.0, 1, 0, 7.25]]))
print(model.predict(X[:5]))
print(y[:5])

We can get a sense of how good our model is by counting the number of datapoints it predicts correctly. This is called the accuracy score.

Let’s create an array that has the predicted y values.
y_pred = model.predict(y)

Now we create an array of boolean values of whether or not our model predicted each passenger correctly.
y == y_pred

To get the number of these that are true, we can use the numpy sum method.
print((y == y_pred).sum())
# 714


This means that of the 887 datapoints, the model makes the correct prediction for 714 of them.

To get the percent correct, we divide this by the total number of passengers. 
We get the total number of passengers using the shape attribute.

y.shape[0]

Thus our accuracy score is computed as follows.
print((y == y_pred).sum() / y.shape[0])
# 0.8038331454340474

Thus the model’s accuracy is 80%. In other words, the model makes the correct prediction on 80% of the datapoints.

print(model.score(X, y))
# 0.8038331454340474


import pandas as pd
from sklearn.linear_model import LogisticRegression

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

model = LogisticRegression()
model.fit(X, y)

y_pred = model.predict(X)
print((y == y_pred).sum())
print((y == y_pred).sum() / y.shape[0])
print(model.score(X, y))

* Breast Cancer DataSet

//Since this dataset is built right into scikit-learn so we won’t need to read in a csv.

import pandas as pd
from sklearn.datasets import load_breast_cancer

cancer_data = load_breast_cancer()
print(cancer_data.keys())
print(cancer_data['DESCR'])		// It is for description


import pandas as pd
from sklearn.datasets import load_breast_cancer

cancer_data = load_breast_cancer()

df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names']) 	// We create our own dataset of some features
df['target'] = cancer_data['target']
print(df.head())



import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.linear_model import LogisticRegression

cancer_data = load_breast_cancer()
df = pd.DataFrame(cancer_data['data'], columns=cancer_data['feature_names'])
df['target'] = cancer_data['target']

X = df[cancer_data.feature_names].values
y = df['target'].values

model = LogisticRegression(solver='liblinear')
model.fit(X, y)
print("prediction for datapoint 0:", model.predict([X[0]]))
print(model.score(X, y))




** Accuracy **

If you have 100 datapoints and predict 70 of them correctly and 30 incorrectly, the accuracy is 70%.

Accuracy is a very straightforward and easy to understand metric, however it’s not always the best one. 
For example, let’s say I have a model to predict whether a credit card charge is fraudulent. 
Of 10000 credit card chards, we have 9900 legitimate charges and 100 fraudulent charges.
I could build a model that just predicts that every single charge is legitimate and it would get 9900/10000 (99%) of the predictions correct!

Accuracy is a good measure if our classes are evenly split, but is very misleading if we have imbalanced classes.


** Confusion Matrix **


We can see all the important values in what is called the Confusion Matrix (or Error Matrix or Table of Confusion).

The Confusion Matrix is a table showing four values:
• Datapoints we predicted positive that are actually positive
• Datapoints we predicted positive that are actually negative
• Datapoints we predicted negative that are actually positive
• Datapoints we predicted negative that are actually negative

The first and fourth are the datapoints we predicted correctly and the second and third are the datapoints we predicted incorrectly.

A true positive (TP) is a datapoint we predicted positively that we were correct about.
A true negative (TN) is a datapoint we predicted negatively that we were correct about.
A false positive (FP) is a datapoint we predicted positively that we were incorrect about.
A false negative (FN) is a datapoint we predicted negatively that we were incorrect about.


** Precision and recall **

Two commonly used metrics for classification are precision and recall. 
Conceptually, precision refers to the percentage of positive results which are relevant 
And recall to the percentage of positive cases correctly classified.

Precision = Positives predicted correctly / Total positive predictions 
                = TP/(TP+FP)

Recall = Positives predicted correctly / Positive cases
           = TP/(TP+FN)


** F1 score **

Accuracy was an appealing metric because it was a single number. 
Precision and recall are two numbers so it’s not always obvious how to choose between two models 
if one has a higher precision and the other has a higher recall. 
The F1 score is an average of precision and recall so that we have a single score for our model.

Here’s the mathematical formula for the F1 score.

F1 = 2*(Precision*Recall)/(Precision + Recall )

The F1 score is the harmonic mean of the precision and recall values.
	

Scikit-learn has a function built in for each of the metrics that we have introduced. 
We have a separate function for each of the accuracy, precision, recall and F1 score
Each function takes two 1-dimensional numpy arrays: the true values of the target & the predicted values of the target. 
We have the true values of the target and the predicted values of the target. Thus we can use the metric functions as follows.

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
print("accuracy:", accuracy_score(y, y_pred))
print("precision:", precision_score(y, y_pred))
print("recall:", recall_score(y, y_pred))
print("f1 score:", f1_score(y, y_pred))


from sklearn.metrics import confusion_matrix
print(confusion_matrix(y, y_pred))




** Overfitting **
Overfitting is when we perform well on the data the model has already seen, but we don’t perform well on new data.


** Training and testing in sklearn **

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y)

print("whole dataset:", X.shape, y.shape)
print("training set:", X_train.shape, y_train.shape)
print("test set:", X_test.shape, y_test.shape)


model = LogisticRegression()
model.fit(X_train, y_train)
print(model.score(X_test, y_test))
y_pred = model.predict(X_test)
print("accuracy:", accuracy_score(y_test, y_pred))
print("precision:", precision_score(y_test, y_pred))
print("recall:", recall_score(y_test, y_pred))
print("f1 score:", f1_score(y_test, y_pred))

** Random state **

To get the same split every time, we can use the random_state attribute. 
We choose an arbitrary number to give it, and then every time we run the code, we will get the same split.

from sklearn.model_selection import train_test_split
X = [[1, 1], [2, 2], [3, 3], [4, 4]]
y = [0, 0, 1, 1]
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=27)
print('X_train', X_train)
print('X_test', X_test)

The random state is also called a seed.


**Logistic regression threshold

If you recall in Lesson 2, we talked about the trade-off between precision and recall. 
With a Logistic Regression model, we have an easy way of shifting between emphasizing precision and emphasizing recall. 
The Logistic Regression model doesn’t just return a prediction, but it returns a probability value between 0 and 1. 
Typically, we say if the value is >=0.5, we predict the passenger survived, and if the value is <0.5, the passenger didn’t survive. 
However, we could choose any threshold between 0 and 1.

If we make the threshold higher, we’ll have fewer positive predictions, but our positive predictions are more likely to be correct. 
This means that the precision would be higher and the recall lower. 
On the other hand, if we make the threshold lower, we’ll have more positive predictions, 
so we’re more likely to catch all the positive cases. This means that the recall would be higher and the precision lower.
Each choice of a threshold is a different model.
 An ROC (Receiver operating characteristic) Curve is a graph showing all of the possible models and their performance.


** Sensitivity and Specificity
An ROC Curve is a graph of the sensitivity vs. the specificity. 

Sensitivity = recall = Positive predicted correctly / Total positive cases = TP/(TP+FN)

Specificity = Negatives predicted correctly / Total negative cases = TN/(TN+FP)


import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import recall_score, precision_recall_fscore_support
sensitivity_score = recall_score
def specificity_score(y_true, y_pred):
    p, r, f, s = precision_recall_fscore_support(y_true, y_pred)
    return r[0]					// r[0] means specificity and r[1] means sensitivity
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=5)
model = LogisticRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
print("sensitivity:", sensitivity_score(y_test, y_pred))
print("specificity:", specificity_score(y_test, y_pred))


** Adjusting the Logistic Regression Threshold in Sklearn **

When you use scikit-learn’s predict method, you are given 0 and 1 values of the prediction. 
However, behind the scenes the Logistic Regression model is getting a probability value between 0 and 1 for each datapoint
and then rounding to either 0 or 1. 
If we want to choose a different threshold besides 0.5, we’ll want those probability values. 
We can use the predict_proba function to get them.
model.predict_proba(X_test)

The result is a numpy array with 2 values for each datapoint (e.g. [0.78, 0.22]). 
You’ll notice that the two values sum to 1. 
The first value is the probability that the datapoint is in the 0 class (didn’t survive) 
and the second is the probability that the datapoint is in the 1 class (survived). 
We only need the second column of this result, which we can pull with the following numpy syntax.

model.predict_proba(X_test)[:, 1]

Now we just want to compare these probability values with our threshold. 
Say we want a threshold of 0.75. We compare the above array to 0.75. 
This will give us an array of True/False values which will be our array of predicted target values.

y_pred = model.predict_proba(X_test)[:, 1] > 0.75

A threshold of 0.75 means we need to be more confident in order to make a positive prediction. 
This results in fewer positive predictions and more negative predictions.

Now we can use any scikit-learn metrics from before using y_test as our true values and y_pred as our predicted values.
print("precision:", precision_score(y_test, y_pred))
print("recall:", recall_score(y_test, y_pred))


import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score
from sklearn.model_selection import train_test_split
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values
X_train, X_test, y_train, y_test = train_test_split(X, y)
model = LogisticRegression()
model.fit(X_train, y_train)
print("predict proba:")
print(model.predict_proba(X_test))
y_pred = model.predict_proba(X_test)[:, 1] > 0.75
print("precision:", precision_score(y_test, y_pred))
print("recall:", recall_score(y_test, y_pred))


Plotting the ROC Curve using the sklearn

We first use the predict_proba method on the model to get the probabilities. 
Then we call the roc_curve function. 
The roc_curve function returns an array of the false positive rates, an array of the true positive rates and the thresholds. 
The false positive rate is 1-specificity (x-axis) and the true positive rate is another term for the sensitivity (y-axis). 
The threshold values won’t be needed in the graph.


model = LogisticRegression()
model.fit(X_train, y_train)
y_pred_proba = model.predict_proba(X_test)
fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba[:,1])

plt.plot(fpr, tpr)
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('1 - specificity')
plt.ylabel('sensitivity')
plt.show()




import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_auc_score

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'

X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y)

model1 = LogisticRegression()
model1.fit(X_train, y_train)
y_pred_proba1 = model1.predict_proba(X_test)
print("model 1 AUC score:", roc_auc_score(y_test, y_pred_proba1[:, 1]))		// AUC :- Area under the curve
model2 = LogisticRegression()
model2.fit(X_train[:, 0:2], y_train)
y_pred_proba2 = model2.predict_proba(X_test[:, 0:2])
print("model 1 AUC score:", roc_auc_score(y_test, y_pred_proba2[:, 1]))		// AUC :- Area under the curve


The higher the AUC, the better the model


** Training and testing **

import pandas as pd
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
import numpy as np

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

X_train, X_test, y_train, y_test = train_test_split(X, y)

# building the model
model = LogisticRegression()
model.fit(X_train, y_train)

# evaluating the model
y_pred = model.predict(X_test)
print(" accuracy: {0:.5f}".format(accuracy_score(y_test, y_pred)))
print("precision: {0:.5f}".format(precision_score(y_test, y_pred)))
print("   recall: {0:.5f}".format(recall_score(y_test, y_pred)))
print(" f1 score: {0:.5f}".format(f1_score(y_test, y_pred)))


** k-fold cross validation **

This process for creating multiple training and test sets is called k-fold cross validation. 
The k is the number of chunks we split our dataset into.
Our goal in cross validation is to get accurate measures for our metrics (accuracy, precision, recall). 
We are building extra models in order to feel confident in the numbers we calculate and report.

** k-fold cross validation using sklearn **

For simplicity, let’s take a dataset with just 6 datapoints and 2 features and a 3-fold cross validation on the dataset. 
We’ll take the first 6 rows from the Titanic dataset and use just the Age and Fare columns.
X = df[['Age', 'Fare']].values[:6]
y = df['Survived'].values[:6]

We start by instantiating a KFold class object. 
It takes two parameters: n_splits (this is k, the number of chunks to create) and shuffle (whether or not to randomize the order of the data). It’s generally good practice to shuffle the data since you often get a dataset that’s in a sorted order.
kf = KFold(n_splits=3, shuffle=True)

The KFold class has a split method that creates the 3 splits for our data.

Let’s look at the output of the split method. The split method returns a generator, so we use the list function to turn it into a list.
list(kf.split(X))



from sklearn.model_selection import KFold
import pandas as pd
df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
X = df[['Age', 'Fare']].values[:6]
y = df['Survived'].values[:6]

kf = KFold(n_splits=3, shuffle=True)

splits = list(kf.split(X))
first_split = splits[0]
train_indices, test_indices = first_split
print("training set indices:", train_indices)
print("test set indices:", test_indices)

X_train = X[train_indices]
X_test = X[test_indices]
y_train = y[train_indices]
y_test = y[test_indices]
print("X_train")
print(X_train)
print("y_train", y_train)
print("X_test")
print(X_test)
print("y_test", y_test)

from sklearn.model_selection import KFold
import pandas as pd

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
X = df[['Age', 'Fare']].values[:6]
y = df['Survived'].values[:6]

kf = KFold(n_splits=3, shuffle=True)
for train, test in kf.split(X):
    print(train, test)



** Building a model **

from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
import pandas as pd

df = pd.read_csv('https://sololearn.com/uploads/files/titanic.csv')
df['male'] = df['Sex'] == 'male'
X = df[['Pclass', 'male', 'Age', 'Siblings/Spouses', 'Parents/Children', 'Fare']].values
y = df['Survived'].values

kf = KFold(n_splits=5, shuffle=True)

splits = list(kf.split(X))
train_indices, test_indices = splits[0]
X_train = X[train_indices]
X_test = X[test_indices]
y_train = y[train_indices]
y_test = y[test_indices]

model = LogisticRegression()
model.fit(X_train, y_train)
print(model.score(X_test, y_test))


** Loop over all the folds **

scores = []
kf = KFold(n_splits=5, shuffle=True)
for train_index, test_index in kf.split(X):
    X_train, X_test = X[train_index], X[test_index]
    y_train, y_test = y[train_index], y[test_index]
    model = LogisticRegression()
    model.fit(X_train, y_train)
    scores.append(model.score(X_test, y_test))
print(scores)

print(np.mean(scores))

final_model = LogisticRegression()
final_model.fit(X, y)





** Decision Tree **

In Logistic Regression, we look at the data graphically and draw a line to separate the data. 
The model is defined by the coefficients that define the line. 
These coefficients are called parameters. 
Since the model is defined by these parameters, Logistic Regression is a parametric machine learning algorithm.

Decision Trees, are example of a nonparametric machine learning algorithm. 
Decision Trees won’t be defined by a list of parameters

In order to determine which feature we should split on first, 
we need to score every possible split so we can choose the split with the highest score. 
Our goal would be to perfectly split the data. 
If, for instance, all women survived the crash and all men didn’t survive, splitting on Sex would be a perfect split. 
This is rarely going to happen with a real dataset, but we want to get as close to this as possible.

The mathematical term we’ll be measuring is called information gain. 
This will be a value from 0 to 1 where 0 is the information gain of a useless split and 1 is the information gain of a perfect split. 
In the next couple parts we will define gini impurity and entropy which we will use to define information gain.


** Gini Impurity **
We calculate the gini impurity on a subset of our data based on how many datapoints in the set are passengers that survived 
and how many are passengers who didn’t survive. 
It will be a value between 0 and 0.5 where 0.5 is completely impure (50% survived and 50% didn’t survive) 
and 0 is completely pure (100% in the same class).

Let's say the percentage of passengers who survived be 'p' and the percentage of passengers who didn't survive be (1-p)
Gini impurity = 2*p*(1-p)


** Entropy **
Entropy is another measure of purity. 
It will be a value between 0 and 1 where 1 is completely impure (50% survived and 50% didn’t survive) and 
0 is completely pure (100% the same class).

entropy = -[plogp + (1-p)log(1-p)]	// Base of the log is 2


** Information Gain **
Now that we have a way of calculating a numerical value for impurity, we can define information gain.
Information Gain = H(S) - (|A| / |S|)*H(A_) - (|B| / |S|)*H(B)
where H is our impurity measure (either Gini impurity or entropy),
|A| means the size of A,
S is the original dataset and A and B are the two sets we’re splitting the dataset S into. 
If we split according to the age, A is passengers with Age<=30 and B is passengers with Age>30. 
If we split according to the sex, A is female passengers and B is male passengers. 

The bigger the value of Information gain, the better the split

from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier()
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=22)
model.fit(X_train, y_train)

We can use the predict method to see what the model predicts. 
Here we can see the prediction for a male passenger in Pclass 3, who is 22 years old, 
has 1 sibling/spouse on board, has 0 parents/children on board, and paid a fare of 7.25.

print(model.predict([[3, True, 22, 1, 0, 7.25]]))

print("accuracy:", model.score(X_test, y_test))
y_pred = model.predict(X_test)
print("precision:", precision_score(y_test, y_pred))
print("recall:", recall_score(y_test, y_pred))




dt = DecisionTreeClassifer(criterion='entropy')

kf = KFold(n_splits=5, shuffle=True)
for criterion in ['gini', 'entropy']:
    print("Decision Tree - {}".format(criterion))
    accuracy = []
    precision = []
    recall = []
    for train_index, test_index in kf.split(X):
        X_train, X_test = X[train_index], X[test_index]
        y_train, y_test = y[train_index], y[test_index]
        dt = DecisionTreeClassifier(criterion=criterion)
        dt.fit(X_train, y_train)
        y_pred = dt.predict(X_test)
        accuracy.append(accuracy_score(y_test, y_pred))
        precision.append(precision_score(y_test, y_pred))
        recall.append(recall_score(y_test, y_pred))
    print("accuracy:", np.mean(accuracy))
    print("precision:", np.mean(precision))
    print("recall:", np.mean(recall))



** Visualizing Decision Trees **
pip install graphviz


from sklearn.tree import export_graphviz
dot_file = export_graphviz(dt, feature_names=feature_names)


import graphviz
graph = graphviz.Source(dot_file)
graph.render(filename='tree', format='png', cleanup=True)


from sklearn.tree import export_graphviz
import graphviz
from IPython.display import Image

feature_names = ['Pclass', 'male']
X = df[feature_names].values
y = df['Survived'].values

dt = DecisionTreeClassifier()
dt.fit(X, y)



**Pruning **

In order to solve these issues, we do what’s called pruning the tree. 
This means we make the tree smaller with the goal of reducing overfitting.

There are two types of pruning: pre-pruning & post-pruning. 
In pre-pruning, we have rules of when to stop building the tree, so we stop building before the tree is too big. 
In post-pruning we build the whole tree and then we review the tree and decide which leaves to remove to make the tree smaller

dot_file = export_graphviz(dt, feature_names=feature_names)
graph = graphviz.Source(dot_file)
graph.render(filename='tree', format='png', cleanup=True)


**Pre-pruning ** 

We’re going to focus on pre-pruning techniques since they are easier to implement. 
We have a few options for how to limit the tree growth.
Here are some commonly used pre-pruning techniques:
• Max depth: Only grow the tree up to a certain depth, or height of the tree. 
	If the max depth is 3, there will be at most 3 splits for each datapoint.
• Leaf size: Don’t split a node if the number of samples at that node is under a threshold
• Number of leaf nodes: Limit the total number of leaf nodes allowed in the tree

Pruning is a balance. 
For example, if you set the max depth too small, you won’t have much of a tree and you won’t have any predictive power. 
This is called underfitting. Similarly if the leaf size is too large, or the number of leaf nodes too small, you’ll have an underfit model.

dt = DecisionTreeClassifier(max_depth=3, min_samples_leaf=2, max_leaf_nodes=10)


from sklearn.model_selection import GridSearchCV

GridSearchCV has four parameters that we’ll use:
1. The model (in this case a DecisionTreeClassifier)
2. Param grid: a dictionary of the parameters names and all the possible values
3. What metric to use (default is accuracy)
4. How many folds for k-fold cross validation

Let’s create the param grid variable. We’ll give a list of all the possible values for each parameter that we want to try.
param_grid = {
    'max_depth': [5, 15, 25],
    'min_samples_leaf': [1, 3],
    'max_leaf_nodes': [10, 20, 35, 50]}

Now we create the grid search object. 
We’ll use the above parameter grid, set the scoring metric to the F1 score, and do a 5-fold cross validation.
dt = DecisionTreeClassifier()
gs = GridSearchCV(dt, param_grid, scoring='f1', cv=5)

Now we can fit the grid search object. This can take a little time to run as it’s trying every possible combination of the parameters.
gs.fit(X, y)

We use the best_params_ attribute to see which model won.
print("best params:", gs.best_params_)



** Random Forest **

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=101)
rf = RandomForestClassifier()
rf.fit(X_train, y_train)
first_row = X_test[0]
print("prediction:", rf.predict([first_row]))
print("true value:", y_test[0])
print("random forest accuracy:", rf.score(X_test, y_test))
dt = DecisionTreeClassifier()
dt.fit(X_train, y_train)
print("decision tree accuracy:", dt.score(X_test, y_test))



rf = RandomForestClassifier(max_features=5)
rf = RandomForestClassifier(n_estimators=15)



 param_grid = {
    'n_estimators': [10, 25, 50, 75, 100],
}

Now we can create a Random Forest Classifier and a Grid Search. Recall that the Grid Search will do k-fold cross validation for us. 
We set cv=5 for 5-fold cross validation.
rf = RandomForestClassifier()
gs = GridSearchCV(rf, param_grid, cv=5)


gs.fit(X, y)
print("best params:", gs.best_params_)

rf = RandomForestClassifier(random_state=123)
gs = GridSearchCV(rf, param_grid, scoring='f1', cv=5)
gs.fit(X, y)
print("best params:", gs.best_params_)





** Elbow Graph **

With a parameter like the number of trees in a random forest, increasing the number of trees will never hurt performance. 
Increasing the number trees will increase performance until a point where it levels out. 
The more trees, however, the more complicated the algorithm. 
A more complicated algorithm is more resource intensive to use. 
Generally it is worth adding complexity to the model if it improves performance but we do not want to unnecessarily add complexity.

We can use what is called an Elbow Graph to find the sweet spot. 
Elbow Graph is a model that optimizes performance without adding unnecessary complexity.

To find the optimal value, let’s do a Grid Search trying all the values from 1 to 100 for n_estimators.
n_estimators = list(range(1, 101))
param_grid = {
    'n_estimators': n_estimators,
}
rf = RandomForestClassifier()
gs = GridSearchCV(rf, param_grid, cv=5)
gs.fit(X, y)

Instead of just looking at the best params like we did before, we are going to use the entire result from the grid search. 
The values are located in the cv_results_ attribute. 
This is a dictionary with a lot of data, however, we will only need one of the keys: mean_test_score. 
Let’s pull out these values and store them as a variable.

scores = gs.cv_results_['mean_test_score']
# [0.91564148, 0.90685413, ...]

Now let’s use matplotlib to graph the results.
import matplotlib.pyplot as plt

scores = gs.cv_results_['mean_test_score']
plt.plot(n_estimators, scores)
plt.xlabel("n_estimators")
plt.ylabel("accuracy")
plt.xlim(0, 100)
plt.ylim(0.9, 1)
plt.show()

If we look at this graph, we see that around 10 trees the graph levels out. 
The best model occurred at n_estimators=33 and n_estimators=64, but given how volatile it is, that was probably due to random chance. 
We should choose about 10 to be our number of estimators, 
because we want the minimum number of estimators that still yield maximum performance.

Now we can build our random forest model with the optimal number of trees.
rf = RandomForestClassifier(n_estimators=10)
rf.fit(X, y) 



** Feature Importances **

There are 30 features in the cancer dataset. 
Does every feature contribute equally to building a model? 
If not, which subset of features should we use? This is a matter of feature selection.

Random forests provide a straightforward method for feature selection: mean decrease impurity. 
Recall that a random forest consists of many decision trees,
 and that for each tree, the node is chosen to split the dataset based on maximum decrease in impurity, 
typically either Gini impurity or entropy in classification. 
Thus for a tree, it can be computed how much impurity each feature decreases in a tree. 
And then for a forest, the impurity decrease from each feature can be averaged. 
Consider this measure a metric of importance of each feature, we then can rank and select the features according to feature importance.

Scikit-learn provides a feature_importances_ variable with the model, which shows the relative importance of each feature. 
The scores are scaled down so that the sum of all scores is 1.
Let's find the feature importances in a random forest with n_estimator = 10 using the training dataset, 
and display them in the descending order.

rf = RandomForestClassifier(n_estimators=10, random_state=111)
rf.fit(X_train, y_train)

ft_imp = pd.Series(rf.feature_importances_, index=cancer_data.feature_names).sort_values(ascending=False)
ft_imp.head(10)

From the output, we can see that among all features, worst radius is most important (0.31), 
followed by mean concave points and worst concave points.
In regression, we calculate the feature importance using variance instead.





** Neural Network Use Cases **

Neural Networks are incredibly popular and powerful machine learning models. 
They often perform well in cases where we have a lot of features as they automatically do feature engineering 
without requiring domain knowledge to restructure the features.

In this module we will be using image data. Since each pixel in the image is a feature, we can have a really large feature set. 
They are all commonly used in text data as it has a large feature set as well. 
Voice recognition is another example where neural networks often shine.
Neural networks often work well without you needing to use domain knowledge to do any feature engineering.
